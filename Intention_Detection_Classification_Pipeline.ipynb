{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intention Detection Classification Pipeline\n",
    "\n",
    "- **Import notebook dependencies**\n",
    "- **Defining notebook variables**\n",
    "- **Defining notebook parameters **\n",
    "- **Feature correlation analysis**\n",
    "- **Creating the classification pipeline**\n",
    "    - data normalization\n",
    "    - features selection\n",
    "    - classification\n",
    "    - gridsearch to optimize hyperparameters\n",
    "    - export model\n",
    "- **Model evaluation**\n",
    "    - examine model accuray, confusion matrix\n",
    "- **Summary of the gridsearch results**\n",
    "    - best model parameters\n",
    "    - best selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import glob\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import random\n",
    "from random import randrange\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# preprocessing\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# feature selection\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "# from yellowbrick.model_selection import RFECV\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# pipeline \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "## explicitly require this experimental feature\n",
    "# from sklearn.experimental import enable_halving_search_cv \n",
    "# from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "# ML models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.evaluate import PredefinedHoldoutSplit # check whether this is used\n",
    "\n",
    "# import/export\n",
    "import joblib\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine window size\n",
    "WIN_SIZE_list = [2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "WIN_SIZE = WIN_SIZE_list[4]\n",
    "\n",
    "# determine the study participant\n",
    "USER_list = ['Mahsa', 'Jaimie']\n",
    "USER = USER_list[0]\n",
    "\n",
    "# choose the feature subsets for clustering\n",
    "featureSet_list = ['ALL', 'ALL_TORQUE', '2D_TORQUE', 'LR_TORQUE', 'LR_TORQUE_MEAN', '2D_TORQUE_MEAN'] \n",
    "featureSet = featureSet_list[1]\n",
    "\n",
    "# number of clusters\n",
    "n_components_list = ['4', '5', '6', '7', '8']\n",
    "n_components = n_components_list[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Parameters (Constant values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to save labeled data and corresponding figures\n",
    "CURR_PATH = os.path.abspath('.')\n",
    "\n",
    "# create a color pallette\n",
    "cmap = matplotlib.cm.get_cmap('tab10')\n",
    "\n",
    "# list of all maneuvers\n",
    "maneuvers = ['Obstacles15', 'Obstacles35', 'RampA', 'StraightF', 'Turn90FR', 'Turn90FL', 'Turn180L', 'Turn180R']\n",
    "\n",
    "# trials\n",
    "trials = ['T1', 'T2', 'T3']\n",
    "\n",
    "# unused columns for correlation analysis\n",
    "unused_cols = ['maneuver', 'trial', 'Prob_L0', 'Prob_L1', 'Prob_L2', 'Prob_L3', 'Prob_L4', 'Prob_L5']\n",
    "\n",
    "# LABELS\n",
    "Labels = ['0', '1', '2', '3', '4', '5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Importing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path to train datasets                         \n",
    "glob_paths = glob.glob(os.path.join(CURR_PATH, 'Labeled_Data', USER, str(WIN_SIZE), '*.csv'))\n",
    "\n",
    "# Read data and update current user dictionary\n",
    "labeled_df = pd.read_csv(glob_paths[0])\n",
    "\n",
    "labeled_df['Clus_label'] = labeled_df['Clus_label'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check some data\n",
    "labeled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_columns = [col for col in labeled_df.columns.tolist() if 'Vel' in col]\n",
    "torque_columns = [col for col in labeled_df.columns.tolist() if 'Torque' in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Split train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "test_trials = []\n",
    "\n",
    "test_df = pd.DataFrame(columns = labeled_df.columns)\n",
    "train_df = pd.DataFrame(columns = labeled_df.columns)\n",
    "\n",
    "for maneuver in maneuvers:\n",
    "    \n",
    "    if 'Ramp' in maneuver and USER == 'Mahsa':\n",
    "        test_tr = trials[randrange(2)]\n",
    "    else:\n",
    "        test_tr = trials[randrange(3)]\n",
    "    \n",
    "    test_trials.append(maneuver+'_'+test_tr)\n",
    "        \n",
    "    df_TE = pd.DataFrame(columns = labeled_df.columns)\n",
    "    df_TE = labeled_df.loc[labeled_df['trial']== maneuver + '_' + test_tr + '_WS' + str(WIN_SIZE) + '_' + USER]\n",
    "    test_df = test_df.append(df_TE, ignore_index=True)\n",
    "    \n",
    "    train_tr = [tr for tr in trials if tr != test_tr]\n",
    "    \n",
    "    for tr in train_tr:\n",
    "        df_TR = pd.DataFrame(columns = labeled_df.columns)\n",
    "        df_TR = labeled_df.loc[labeled_df['trial']== maneuver + '_' + tr + '_WS' + str(WIN_SIZE) + '_' + USER]\n",
    "        train_df = train_df.append(df_TR, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test set shape: {}, train set shape: {}'.format(test_df.shape, train_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Feature correlation analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory to save results\n",
    "results_path = os.path.join(CURR_PATH, 'Results')\n",
    "\n",
    "# get current time to use for saving models/figures\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# create directiry for the current time\n",
    "path_ = os.path.join(results_path, timestr)\n",
    "os.makedirs(path_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_analysis(dataset, target_label):\n",
    "      \n",
    "    df = dataset.copy()\n",
    "    \n",
    "    df = df.drop(columns=unused_cols)\n",
    "        \n",
    "    # calculate correlation matrix\n",
    "    cor = df.corr()\n",
    "    \n",
    "    display(cor.head())\n",
    "    \n",
    "    # calculate correlation with output variable\n",
    "    cor_target = abs(cor[target_label])\n",
    "    cor_target = cor_target.sort_values(ascending=False)\n",
    "    print('feature correlation values with {} target value: \\n{}'.format(target_label, cor_target))\n",
    "    \n",
    "    # save correlation values to csv\n",
    "    filename = 'correlation_matrix.csv'\n",
    "    filename = os.path.join(path_, filename)\n",
    "    cor_target.to_csv(filename)\n",
    "    \n",
    "    # drop columns associated with data labels\n",
    "    cor = cor.drop(['Clus_label'], axis = 1)\n",
    "    cor = cor.drop(['Clus_label'], axis = 0)\n",
    "    \n",
    "    # plot heat map\n",
    "    plt.figure(figsize = (16,12))\n",
    "    sns.heatmap(abs(cor), cmap = plt.cm.Reds)\n",
    "    \n",
    "    # save correlation heatmap\n",
    "    fig_name = 'correlation_analysis.jpg'\n",
    "    fig_name = os.path.join(path_, fig_name)\n",
    "    plt.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correlation_analysis(train_df, 'Clus_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Classification Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to Create TRAIN/VALIDATE/TEST data sets\n",
    "def create_train_test(dataset, target_label, test_size):  \n",
    "    '''\n",
    "    input: get dataset and desired target label to perform classification\n",
    "    output: train/test splits\n",
    "    '''\n",
    "    df = dataset.copy()\n",
    "    \n",
    "    df = df.drop(columns=unused_cols)\n",
    "    \n",
    "    y = df.pop(target_label)\n",
    "        \n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=test_size, random_state=0, shuffle=False)\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train set\n",
    "X_train, X_, y_train, y_ = create_train_test(train_df, 'Clus_label', test_size = 1)\n",
    "\n",
    "print('X_train shape = {} , y_train shape = {}'.format(X_train.shape, y_train.shape))\n",
    "print('X_ shape = {} , y_ shape = {}'.format(X_.shape, y_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create test set\n",
    "X_, X_test, y_, y_test = create_train_test(test_df, 'Clus_label', test_size = len(test_df)-1)\n",
    "\n",
    "print('X_ shape = {} , y_ shape = {}'.format(X_.shape, y_.shape))\n",
    "print('X_test shape = {} , y_test shape = {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train[torque_columns]\n",
    "# X_test = X_test[torque_columns]\n",
    "\n",
    "# print('X_train shape = {} , y_train shape = {}'.format(X_train.shape, y_train.shape))\n",
    "# print('X_test shape = {} , y_test shape = {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Create Classification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_pipeline(X_train, y_train, HalvGrid = False):\n",
    "    \n",
    "    # cross validation nfolds\n",
    "    cv = 5\n",
    "    \n",
    "    # classifier to use for feature selection\n",
    "    feat_selection_clf = RandomForestClassifier(n_estimators=50, max_depth=10, n_jobs=-1, random_state = 0)\n",
    "    \n",
    "    # create pipeline\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('selector', SFS(estimator = feat_selection_clf, \n",
    "                                      k_features=(2, X_train.shape[1]), \n",
    "                                      forward=True, \n",
    "                                      floating=False, \n",
    "                                      scoring='accuracy', \n",
    "                                      cv=cv, \n",
    "                                      n_jobs=-1)),\n",
    "                     ('classifier', RandomForestClassifier())])\n",
    "    \n",
    "    # set parameter grid\n",
    "    param_grid = [\n",
    "        {'selector':[SFS(estimator = feat_selection_clf)],\n",
    "                        'selector__estimator__n_estimators':[50],\n",
    "                        'selector__estimator__max_depth':[3,5,10]},\n",
    "\n",
    "#         {'selector':[RFE(estimator= feat_selection_clf)],\n",
    "#                         'selector__estimator__n_estimators':[5, 10],\n",
    "#                         'selector__estimator__max_depth':[3,4], \n",
    "#          'selector__n_features_to_select':[1,2]},\n",
    "        \n",
    "#         {'selector':[RFECV(estimator = feat_selection_clf, min_features_to_select = 1)],\n",
    "#                          'selector__estimator__n_estimators':[5, 10],\n",
    "#                          'selector__estimator__max_depth':[3,4]},\n",
    "        \n",
    "        {'classifier':[RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state = 0)],\n",
    "         'classifier__n_estimators':[30,50],\n",
    "         'classifier__max_depth':[5,10,15]}]\n",
    "    \n",
    "    # dictionary of evaluation scores\n",
    "    scoring = {\n",
    "        'precision_score': make_scorer(precision_score, average='macro'),\n",
    "        'recall_score': make_scorer(recall_score, average='macro'),\n",
    "        'accuracy_score': make_scorer(accuracy_score),\n",
    "        'f1_score':make_scorer(f1_score, average='macro')}\n",
    "\n",
    "    # gridsearch \n",
    "    if HalvGrid:\n",
    "        # HalvingGridSearch\n",
    "        grid = HalvingGridSearchCV(pipe, \n",
    "                                   param_grid, \n",
    "                                   cv = cv, \n",
    "                                   scoring = scoring,\n",
    "                                   refit ='accuracy_score',\n",
    "                                   return_train_score=True,\n",
    "                                   random_state = 0,\n",
    "                                   n_jobs = -1,\n",
    "                                   verbose = 0)\n",
    "    else:\n",
    "        grid = GridSearchCV(pipe, \n",
    "                            param_grid, \n",
    "                            scoring = scoring,\n",
    "                            n_jobs = -1,\n",
    "                            refit ='accuracy_score',\n",
    "                            cv = cv, \n",
    "                            verbose = 0,\n",
    "                            return_train_score=True)\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    return grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the classification pipeline\n",
    "model_ = clf_pipeline(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parameters of the best model\n",
    "print(\"Best estimator via GridSearch \\n\", model_.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name/directory\n",
    "model_name = 'model.joblib'\n",
    "model_name = os.path.join(path_, model_name)\n",
    "\n",
    "# dump model\n",
    "joblib.dump(model_, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Evaluation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "print('Accuracy score = {:0.2f}'.format(model_.score(X_test, y_test) * 100, '.2f'))\n",
    "\n",
    "y_pred = model_.predict(X_test)\n",
    "\n",
    "# f1_score\n",
    "f_score = f1_score(y_test, y_pred, average = 'macro')* 100\n",
    "print('F1-score = {:0.2f}'.format(f_score))\n",
    "\n",
    "# recall\n",
    "recall_ = recall_score(y_test, y_pred, average = 'macro')* 100\n",
    "print('Recall score = {:0.2f}'.format(recall_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "title = \"Normalized confusion matrix\"\n",
    "\n",
    "disp = plot_confusion_matrix(model_, X_test, y_test, display_labels=Labels,\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             normalize='true',\n",
    "                             xticks_rotation = 45,\n",
    "                             values_format = '.2f')\n",
    "disp.ax_.set_title(title)\n",
    "disp.ax_.grid(False)\n",
    "disp.figure_.set_size_inches(12,12)\n",
    "\n",
    "# save confusion matrix\n",
    "fig_name = 'confusion_matrix.jpg'\n",
    "fig_name = os.path.join(path_, fig_name)\n",
    "plt.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Analyze computational performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # computational performance\n",
    "# X_Test_test = X_test[:1].copy()\n",
    "\n",
    "# # method 1\n",
    "# %timeit y_pred = model_.predict(X_Test_test)\n",
    "\n",
    "# # method 2\n",
    "# time1 = time.time()\n",
    "# y_pred = model_.predict(X_Test_test)\n",
    "# time2 = time.time()\n",
    "# print('prediction time: {} ms'.format((time2-time1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - GridSearch Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Model best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean cross-validated score of the best_estimator\n",
    "print('Best feature combination had a CV accuracy of:', model_.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Model best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters via GridSearch \\n\", model_.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Visualize GridSearch results\n",
    "##### Source: https://www.kaggle.com/grfiv4/displaying-the-results-of-a-grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def GridSearch_table_plot(grid_clf, param_name,\n",
    "#                           num_results=4,\n",
    "#                           negative=True,\n",
    "#                           graph=True,\n",
    "#                           display_all_params=False):\n",
    "\n",
    "#     '''Display grid search results\n",
    "\n",
    "#     Arguments\n",
    "#     ---------\n",
    "\n",
    "#     grid_clf           the estimator resulting from a grid search\n",
    "#                        for example: grid_clf = GridSearchCV( ...\n",
    "\n",
    "#     param_name         a string with the name of the parameter being tested\n",
    "\n",
    "#     num_results        an integer indicating the number of results to display\n",
    "#                        Default: 15\n",
    "\n",
    "#     negative           boolean: should the sign of the score be reversed?\n",
    "#                        scoring = 'neg_log_loss', for instance\n",
    "#                        Default: True\n",
    "\n",
    "#     graph              boolean: should a graph be produced?\n",
    "#                        non-numeric parameters (True/False, None) don't graph well\n",
    "#                        Default: True\n",
    "\n",
    "#     display_all_params boolean: should we print out all of the parameters, not just the ones searched for?\n",
    "#                        Default: True\n",
    "\n",
    "#     Usage\n",
    "#     -----\n",
    "\n",
    "#     GridSearch_table_plot(grid_clf, \"min_samples_leaf\")\n",
    "\n",
    "#                           '''\n",
    "#     from matplotlib      import pyplot as plt\n",
    "#     from IPython.display import display\n",
    "#     import pandas as pd\n",
    "\n",
    "#     clf = grid_clf.best_estimator_\n",
    "#     clf_params = grid_clf.best_params_\n",
    "#     if negative:\n",
    "#         clf_score = -grid_clf.best_score_\n",
    "#     else:\n",
    "#         clf_score = grid_clf.best_score_\n",
    "    \n",
    "#     clf_stdev = grid_clf.cv_results_['std_test_accuracy_score'][grid_clf.best_index_]\n",
    "#     cv_results = grid_clf.cv_results_\n",
    "\n",
    "#     print(\"best parameters: {}\".format(clf_params))\n",
    "#     print(\"best score:      {:0.5f} (+/-{:0.5f})\".format(clf_score, clf_stdev))\n",
    "#     if display_all_params:\n",
    "#         import pprint\n",
    "#         pprint.pprint(clf.get_params())\n",
    "\n",
    "#     # pick out the best results\n",
    "#     # =========================\n",
    "#     scores_df = pd.DataFrame(cv_results).sort_values(by='rank_test_accuracy_score')\n",
    "\n",
    "#     best_row = scores_df.iloc[0, :]\n",
    "#     if negative:\n",
    "#         best_mean = -best_row['mean_test_accuracy_score']\n",
    "#     else:\n",
    "#         best_mean = best_row['mean_test_accuracy_score']\n",
    "#     best_stdev = best_row['std_test_accuracy_score']\n",
    "#     best_param = best_row['param_' + param_name]\n",
    "\n",
    "#     # display the top 'num_results' results\n",
    "#     # =====================================\n",
    "#     display(pd.DataFrame(cv_results) \\\n",
    "#             .sort_values(by='rank_test_accuracy_score').head(num_results))\n",
    "\n",
    "#     # plot the results\n",
    "#     # ================\n",
    "#     scores_df = scores_df.sort_values(by='param_' + param_name)\n",
    "\n",
    "#     if negative:\n",
    "#         means = -scores_df['mean_test_accuracy_score']\n",
    "#     else:\n",
    "#         means = scores_df['mean_test_accuracy_score']\n",
    "#     stds = scores_df['std_test_accuracy_score']\n",
    "#     params = scores_df['param_' + param_name]\n",
    "    \n",
    "#     # plot\n",
    "#     if graph:\n",
    "#         plt.figure(figsize=(8, 8))\n",
    "#         plt.errorbar(params, means, yerr=stds)\n",
    "\n",
    "#         plt.axhline(y=best_mean + best_stdev, color='red')\n",
    "#         plt.axhline(y=best_mean - best_stdev, color='red')\n",
    "#         plt.plot(best_param, best_mean, 'or')\n",
    "\n",
    "#         plt.title(param_name + \" vs Score\\nBest Score {:0.5f}\".format(clf_score))\n",
    "#         plt.xlabel(param_name)\n",
    "#         plt.ylabel('Score')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GridSearch_table_plot(model_, \"selector__estimator__n_estimators\", negative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Gridsearch evaluation - multiple scores\n",
    "##### source: https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score_evaluation(grid, param):\n",
    "\n",
    "#     # plotting the results\n",
    "#     results = grid.cv_results_\n",
    "    \n",
    "#     scoring = {\n",
    "#             'precision_score': make_scorer(precision_score, average='macro'),\n",
    "#             'recall_score': make_scorer(recall_score, average='macro'),\n",
    "#             'accuracy_score': make_scorer(accuracy_score),\n",
    "#             'f1_score':make_scorer(f1_score, average='macro')}\n",
    "\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n",
    "#               fontsize=16)\n",
    "\n",
    "#     plt.xlabel(param)\n",
    "#     plt.ylabel(\"Score\")\n",
    "\n",
    "#     ax = plt.gca()\n",
    "#     ax.set_xlim(0, 100)\n",
    "#     ax.set_ylim(0.0, 1)\n",
    "\n",
    "#     # Get the regular numpy array from the MaskedArray\n",
    "#     X_axis = np.array(results['param_selector__estimator__n_estimators'].data, dtype=float)\n",
    "\n",
    "#     for scorer, color in zip(sorted(scoring), ['g', 'k','r','b']):\n",
    "#         for sample, style in (('train', '--'), ('test', '-')):\n",
    "#             sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
    "#             sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
    "#             ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "#                             sample_score_mean + sample_score_std,\n",
    "#                             alpha=0.1 if sample == 'test' else 0, color=color)\n",
    "#             ax.plot(X_axis, sample_score_mean, style, color=color,\n",
    "#                     alpha=1 if sample == 'test' else 0.7,\n",
    "#                     label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "#         best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "#         best_score = results['mean_test_%s' % scorer][best_index]\n",
    "\n",
    "#         # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "#         ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "#                 linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "#         # Annotate the best score for that scorer\n",
    "#         ax.annotate(\"%0.2f\" % best_score,\n",
    "#                     (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "#     plt.legend(loc=\"best\")\n",
    "#     plt.grid(False)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_evaluation(model_, \"classifier__max_depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Gridsearch report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# results = model_.cv_results_\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 - Feature selection summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Feature selection score & best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction score of best selected features\n",
    "print('\\nFeature selection score: {}'.format(model_.best_estimator_['selector'].k_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best features\n",
    "best_feats_idx = model_.best_estimator_['selector'].k_feature_idx_;\n",
    "\n",
    "best_feats = list(X_test.columns[list(best_feats_idx)].values.tolist()) \n",
    "\n",
    "print('\\nBest features: \\n{}'.format(best_feats))\n",
    "\n",
    "# save a list of selected features\n",
    "filename = 'selected_features.csv'\n",
    "filename = os.path.join(path_, filename)\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([c.strip() for c in r.strip(', ').split(',')] for r in best_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test shape of the feature transformed dataframe\n",
    "X_test_transformed = model_.best_estimator_['selector'].transform(X_test)\n",
    "print('shape of the transformed dataset with best features: {}'.format(X_test_transformed.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Visualize feature selection scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting feature selection characteristics\n",
    "plot_sfs(model_.best_estimator_['selector'].get_metric_dict(), kind='std_err', figsize=(12,5))\n",
    "plt.title('Sequential Forward Selection (w. StdErr)')\n",
    "plt.grid(b=True, which='major', axis='both')\n",
    "\n",
    "# save confusion matrix\n",
    "fig_name = 'feature_selection.jpg'\n",
    "fig_name = os.path.join(path_, fig_name)\n",
    "plt.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the index range for each maneuver\n",
    "def range_dic_(df_):\n",
    "    \"\"\"\n",
    "    get the start index of each maneuver from the original dataframe\n",
    "    \"\"\"\n",
    "    range_dic = {}\n",
    "    for man in df_['maneuver']:\n",
    "        trial_indx = df_.index[df_['maneuver'] == man].tolist()\n",
    "        range_ = (min(trial_indx), max(trial_indx))\n",
    "        range_dic.update({man:range_})\n",
    "    return range_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_range = range_dic_(test_df)\n",
    "man_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot clusters in time series data\n",
    "def plt_ts_cluster_prediction(df_clus_, predictions, clusterNum, features_to_plot, man_type = 'All', zoom = False):\n",
    "    \"\"\"\n",
    "    input: input original dataframe (with maneuver columns), clustered dataframe, number of clusteres, \n",
    "           and selected features to plot\n",
    "    output: plotting clustered time series data with different colors\n",
    "    \"\"\"\n",
    "    \n",
    "    MARKER_SIZE =10\n",
    "    LINE_WIDTH = 5\n",
    "    color_dict = {}\n",
    "    \n",
    "    plt_num = len(features_to_plot)\n",
    "    \n",
    "    fig, axs = plt.subplots(plt_num, 1, figsize=(15,15), constrained_layout=True)\n",
    "    axs = axs.ravel()\n",
    "    \n",
    "    if man_type != 'All':\n",
    "        df_clus = df_clus_[man_range[man_type][0]:man_range[man_type][1]]\n",
    "\n",
    "    true_states = df_clus['Clus_label'].astype(int)\n",
    "    predicted_states = predictions[man_range[man_type][0]:man_range[man_type][1]].astype(int)\n",
    "    \n",
    "    colors = [cmap(i) for i in range(clusterNum)]\n",
    "        \n",
    "    for i in range(clusterNum):\n",
    "        color_dict.update({i:colors[i]})\n",
    "        \n",
    "    color_array_true = [color_dict[i] for i in true_states]\n",
    "    color_array_predicted = [color_dict[i] for i in predicted_states]\n",
    "    \n",
    "    for i, feature in enumerate(features_to_plot):    \n",
    "        axs[i].grid()\n",
    "        axs[i].scatter(range(len(df_clus)), df_clus[feature], facecolors='none', edgecolors=color_array_true, \n",
    "                       linewidth=2*LINE_WIDTH, s = 5*MARKER_SIZE)\n",
    "        axs[i].scatter(range(len(df_clus)), df_clus[feature], c=color_array_predicted, s = 5*MARKER_SIZE, \n",
    "                       marker = 'o')\n",
    "        axs[i].set_ylabel(feature+ ' (Nm)', fontsize=25)\n",
    "        axs[i].tick_params(direction='out', labelsize = 25)\n",
    "        axs[i].set_xlim((0, len(df_clus)))\n",
    "\n",
    "        if zoom:        \n",
    "            from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "            # create the zoomed in version\n",
    "            axins = zoomed_inset_axes(axs[i], 3, loc=1) # zoom-factor: 3, location: upper-left\n",
    "            axins.scatter(range(len(df_clus)), df_clus[feature], facecolors='none', edgecolors=color_array_true, \n",
    "                           linewidth=5, s = 20*MARKER_SIZE)\n",
    "            axins.scatter(range(len(df_clus)), df_clus[feature], c=color_array_predicted, s = 10*MARKER_SIZE, \n",
    "                           marker = 'o')\n",
    "            x1, x2, y1, y2 = 25, 55, -1, 1 # specify the limits\n",
    "            axins.set_xlim(x1, x2) # apply the x-limits\n",
    "            axins.set_ylim(y1, y2) # apply the y-limits\n",
    "            plt.yticks(visible=False)\n",
    "            plt.xticks(visible=False)\n",
    "\n",
    "            from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "            mark_inset(axs[i], axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "        \n",
    "        # dummy scatter for labels\n",
    "        axs[i].scatter([], [], edgecolors=colors[0], label='Recovery (True)', facecolors='none',\n",
    "                       linewidth=LINE_WIDTH, s = 20*MARKER_SIZE,)\n",
    "        axs[i].scatter([], [], edgecolors=colors[1], label='Right-assist (True)', facecolors='none', \n",
    "                       linewidth=LINE_WIDTH, s = 20*MARKER_SIZE,)\n",
    "        axs[i].scatter([], [], edgecolors=colors[2], label='Straight-assist (True)', facecolors='none', \n",
    "                       linewidth=LINE_WIDTH, s = 20*MARKER_SIZE,)\n",
    "        axs[i].scatter([], [], edgecolors=colors[3], label='Release (True)', facecolors='none', \n",
    "                       linewidth=LINE_WIDTH, s = 20*MARKER_SIZE,)\n",
    "        axs[i].scatter([], [], edgecolors=colors[4], label='Braking (True)', facecolors='none', \n",
    "                       linewidth=LINE_WIDTH, s = 20*MARKER_SIZE,)\n",
    "        axs[i].scatter([], [], edgecolors=colors[5], label='Left-assist (True)', facecolors='none', \n",
    "                       linewidth=LINE_WIDTH, s = 20*MARKER_SIZE,)\n",
    "        \n",
    "        axs[i].scatter([], [], c=np.array(colors[0]).reshape(1,-1), marker = 'o', label='Recovery (Predicted)', s = 10*MARKER_SIZE)\n",
    "        axs[i].scatter([], [], c=np.array(colors[1]).reshape(1,-1), marker = 'o', label='Right-assist (Predicted)', s = 10*MARKER_SIZE)\n",
    "        axs[i].scatter([], [], c=np.array(colors[2]).reshape(1,-1), marker = 'o', label='Straight-assist (Predicted)', s = 10*MARKER_SIZE)\n",
    "        axs[i].scatter([], [], c=np.array(colors[3]).reshape(1,-1), marker = 'o', label='Release (Predicted)', s = 10*MARKER_SIZE)\n",
    "        axs[i].scatter([], [], c=np.array(colors[4]).reshape(1,-1), marker = 'o', label='Braking (Predicted)', s = 10*MARKER_SIZE)\n",
    "        axs[i].scatter([], [], c=np.array(colors[5]).reshape(1,-1), marker = 'o', label='Left-assist (Predicted)', s = 10*MARKER_SIZE)\n",
    "        \n",
    "    axs[i].legend(ncol=2, fontsize = 10)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # save confusion matrix\n",
    "    fig_name = man_type + '_Classification_Clustering.jpg'\n",
    "    fig_name = os.path.join(path_, fig_name)\n",
    "    fig.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_plot= ['Mean Torque_L', 'Mean Torque_R']\n",
    "plt_ts_cluster_prediction(test_df, y_pred, int(n_components), features_to_plot, man_type = 'StraightF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_plot= ['Mean Torque_L', 'Mean Torque_R']\n",
    "plt_ts_cluster_prediction(test_df, y_pred, int(n_components), features_to_plot, man_type = 'Turn90FR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_plot= ['Mean Torque_L', 'Mean Torque_R']\n",
    "plt_ts_cluster_prediction(test_df, y_pred, int(n_components), features_to_plot, man_type = 'Turn90FL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detemine label probabilities\n",
    "labels_prob = model_.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine label probabilities\n",
    "color_ = [cmap(i) for i in range(int(n_components))]\n",
    "\n",
    "plt.figure(figsize = (18, 10))\n",
    "\n",
    "for n in range(int(n_components)):\n",
    "    test_df['Prob_L'+str(n)] = labels_prob[:,n]\n",
    "    \n",
    "for i in range(int(n_components)):\n",
    "    plt.plot(test_df['Prob_L'+str(i)], label = str(i), c = color_[i])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save labeled dataset\n",
    "filename = 'Labeled_classification.csv'\n",
    "filename = os.path.join(path_, filename)\n",
    "test_df.to_csv(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
